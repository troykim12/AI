# -*- coding: utf-8 -*-
"""concatensemble(deit&resnet101).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfOTeeU-_gc6DbvmI_KMp8_KzL3jIiFb
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from transformers import DeiTModel, DeiTConfig, DeiTFeatureExtractor
from torch.utils.data import DataLoader
from sklearn.metrics import f1_score
from tqdm import tqdm

!pip install Lion

!pip install lion_pytorch

from google.colab import drive
drive.mount('/content/drive')
#구글 코랩으로 마운트해서 했으니까 로컬에서는 필요없을거임

!unzip -q "/content/drive/MyDrive/건설용 자갈 분류 DACON/train.zip" -d "/content/gravel_data"

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torch.utils.data import DataLoader, Subset

# 1️⃣ Transform 정의
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])

# 2️⃣ ImageFolder로 전체 데이터셋 로드
dataset = ImageFolder(root="/content/gravel_data/train", transform=transform)

# 3️⃣ 클래스 이름 및 개수 자동 추출
class_names = dataset.classes
num_classes = len(class_names)
print(f"📦 클래스 수: {num_classes}")
print(f"🧷 클래스 목록: {class_names}")

# 4️⃣ 라벨 목록 추출 (stratify용)
labels = np.array(dataset.targets)

# 6️⃣ Stratified train/val split
train_idx, val_idx = train_test_split(
    np.arange(len(labels)),
    test_size=0.1,
    stratify=labels,
    random_state=42
)

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)

# 7️⃣ DataLoader 생성
train_loader = DataLoader(train_dataset, batch_size=70, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=70, shuffle=False, num_workers=4, pin_memory=True)

# 8️⃣ 분할 결과 출력
print(f"✅ Train: {len(train_dataset)}개 / Val: {len(val_dataset)}개")

from collections import Counter

train_labels = [labels[i] for i in train_idx]
val_labels = [labels[i] for i in val_idx]

print("Train 클래스 분포:", Counter(train_labels))
print("Val 클래스 분포:", Counter(val_labels))

# ✅ 이어서 학습을 위한 전체 코드 템플릿 (DeiT + ResNet101 ConcatEnsemble)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR
from sklearn.metrics import f1_score
from tqdm import tqdm
import timm

# ⚙️ 기본 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = 7

# 📁 데이터 로더는 사전에 정의되어 있어야 함
# 예시: train_loader, val_loader = get_dataloaders(...)

# 🔹 ResNet101 Feature Extractor
resnet_base = models.resnet101(pretrained=False)
resnet_ckpt = torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_model_resnet101.pth")
resnet_ckpt = {k: v for k, v in resnet_ckpt.items() if not k.startswith("fc.")}
resnet_base.load_state_dict(resnet_ckpt, strict=False)

class ResNetFeatureExtractor(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.backbone = nn.Sequential(*list(base_model.children())[:-1])
        self.out_dim = base_model.fc.in_features

    def forward(self, x):
        x = self.backbone(x)
        return torch.flatten(x, 1)

resnet = ResNetFeatureExtractor(resnet_base).to(device)

# 🔹 DeiT Feature Extractor
state_dict = torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_deit_base.pth")
state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}

deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0)
deit.load_state_dict(state_dict)
deit.out_dim = deit.num_features

class DeiTFeatureExtractor(nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.out_dim = backbone.num_features

    def forward(self, x):
        return self.backbone(x)

deit = DeiTFeatureExtractor(deit).to(device)

# 🔹 Concat Ensemble
class ConcatEnsemble(nn.Module):
    def __init__(self, resnet, deit, num_classes):
        super().__init__()
        self.resnet = resnet
        self.deit = deit
        self.classifier = nn.Sequential(
            nn.Linear(resnet.out_dim + deit.out_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        res_feat = self.resnet(x)
        deit_feat = self.deit(x)
        return self.classifier(torch.cat((res_feat, deit_feat), dim=1))
state_dict = torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_deit_base.pth")

# 2. classifier 부분 키 제거 → feature extractor 용으로 사용
filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}
# timm 모델 그대로 사용 (backbone wrapper 없음)
deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0)
deit.load_state_dict(filtered_state_dict)
deit.out_dim = deit.num_features
deit = deit.to(device)

# 그대로 앙상블
model = ConcatEnsemble(resnet, deit, num_classes).to(device)

# 이어 학습
model.load_state_dict(torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_concatEnsemble_model.pth"))

start_epoch = 1  # 에포크 1까지 학습되었으면 2부터 시작

# 🔧 Optimizer & Scheduler
optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
warmup_scheduler = LinearLR(optimizer, start_factor=1e-2, total_iters=2 * len(train_loader))

criterion = nn.CrossEntropyLoss()

# 🔁 Training Loop (이어하기 포함)
def train(model, train_loader, val_loader, num_epochs, start_epoch=0):
    best_f1 = 0.0
    step = 0
    save_path = "/content/drive/MyDrive/자갈 train 및 test 사진/best_concatEnsemble_model.pth"

    for epoch in range(start_epoch, num_epochs):
        model.train()
        total_loss, correct, total = 0.0, 0, 0

        for images, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}/{num_epochs}]"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            if epoch < 2:
                warmup_scheduler.step()
            else:
                scheduler.step(epoch + step / len(train_loader))
            step += 1

            total_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_loss = total_loss / total
        train_acc = correct / total
        val_loss, val_acc, val_f1 = evaluate(model, val_loader)

        if val_f1 > best_f1:
            best_f1 = val_f1
            torch.save(model.state_dict(), save_path)
            print(f"\n📦 Saved new best model at Epoch {epoch+1} with F1: {val_f1:.4f}")

        print(f"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

# 🔍 Evaluation 함수
def evaluate(model, val_loader):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    preds, targets = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            pred = outputs.argmax(dim=1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
            preds.extend(pred.cpu().numpy())
            targets.extend(labels.cpu().numpy())

    avg_loss = total_loss / total
    avg_acc = correct / total
    macro_f1 = f1_score(targets, preds, average='macro')
    return avg_loss, avg_acc, macro_f1

# 🟢 이어서 학습 실행
train(model, train_loader, val_loader, num_epochs=20, start_epoch=start_epoch)

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torchvision import models
# from transformers import DeiTModel, DeiTConfig
# from torch.utils.data import DataLoader
# from torch.optim import AdamW
# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
# from sklearn.metrics import f1_score
# from tqdm import tqdm
# import timm

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# num_classes = 7

# resnet_base = models.resnet101(pretrained=False)
# state_dict = torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_model_resnet101.pth")

# # fc 레이어 키 제거
# state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}

# # 수정된 state_dict 로드
# resnet_base.load_state_dict(state_dict, strict=False)


# # 2️⃣ FeatureExtractor 클래스 정의
# class ResNetFeatureExtractor(nn.Module):
#     def __init__(self, base_model):
#         super().__init__()
#         self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # avgpool 전까지
#         self.out_dim = base_model.fc.in_features

#     def forward(self, x):
#         x = self.backbone(x)
#         x = torch.flatten(x, 1)
#         return x

# # 3️⃣ feature extractor에 base 모델 주입
# resnet = ResNetFeatureExtractor(resnet_base).to(device)



# class DeiTFeatureExtractor(nn.Module):
#     def __init__(self, backbone):
#         super().__init__()
#         self.backbone = backbone
#         self.out_dim = backbone.num_features

#     def forward(self, x):
#         return self.backbone(x)


# # 🔹 Concat Ensemble Model
# class ConcatEnsemble(nn.Module):
#     def __init__(self, resnet, deit, num_classes):
#         super().__init__()
#         self.resnet = resnet
#         self.deit = deit
#         self.classifier = nn.Sequential(
#             nn.Linear(resnet.out_dim + deit.out_dim, 512),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(512, num_classes)
#         )

#     def forward(self, x):
#         res_feat = self.resnet(x)
#         deit_feat = self.deit(x)
#         combined = torch.cat((res_feat, deit_feat), dim=1)
#         return self.classifier(combined)

# # 🔹 Instantiate Model,🔹 Load pretrained weights (.pth)
# state_dict = torch.load("/content/drive/MyDrive/자갈 train 및 test 사진/best_deit_base.pth")
# # ❌ head.weight, head.bias 키 제거
# filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}

# deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0).to(device)
# deit.load_state_dict(filtered_state_dict)

# # ✅ 이 한 줄로 해결됨!
# deit.out_dim = deit.num_features
# deit = deit.to(device)

# model = ConcatEnsemble(resnet, deit, num_classes).to(device)



# # 🔹 Enable fine-tuning
# for p in model.parameters():
#     p.requires_grad = True

# # 🔹 Optimizer: Lion
# optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)

# # 🔹 LR Scheduler: CosineAnnealingWarmRestarts
# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)

# # 🔹 Warmup Scheduler (manual): optional
# warmup_epochs = 2
# warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
#     optimizer, start_factor=1e-2, total_iters=warmup_epochs * len(train_loader)
# )

# # 🔹 Loss Function
# criterion = nn.CrossEntropyLoss()

# # 🔁 Training Loop
# def train(model, train_loader, val_loader, num_epochs):
#     step = 0
#     best_f1 = 0.0
#     save_path = "/content/drive/MyDrive/자갈 train 및 test 사진/best_concatEnsemble_model.pth"

#     for epoch in range(num_epochs):
#         model.train()
#         train_loss = 0.0
#         correct = 0
#         total = 0

#         for images, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}/{num_epochs}]"):
#             images, labels = images.to(device), labels.to(device)

#             optimizer.zero_grad()
#             outputs = model(images)
#             loss = criterion(outputs, labels)
#             loss.backward()
#             optimizer.step()

#             # Scheduler & Warmup
#             if epoch < warmup_epochs:
#                 warmup_scheduler.step()
#             else:
#                 scheduler.step(epoch + step / len(train_loader))
#             step += 1

#             train_loss += loss.item() * images.size(0)
#             preds = outputs.argmax(dim=1)
#             correct += (preds == labels).sum().item()
#             total += labels.size(0)

#         epoch_train_loss = train_loss / total
#         epoch_train_acc = correct / total

#         # 🔍 Validation
#         val_loss, val_acc, val_f1 = evaluate(model, val_loader)

#         # ✅ 모델 저장 조건: F1 기준 최고
#         if val_f1 > best_f1:
#             best_f1 = val_f1
#             torch.save(model.state_dict(), save_path)
#             print(f"📦 Saved new best model at Epoch {epoch+1} with F1: {val_f1:.4f}")

#         # 🔢 출력
#         print(f"Epoch {epoch+1:02d} | "
#               f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | "
#               f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")



# def evaluate(model, val_loader):
#     model.eval()
#     total_loss = 0.0
#     correct = 0
#     total = 0
#     preds = []
#     targets = []

#     with torch.no_grad():
#         for images, labels in val_loader:
#             images, labels = images.to(device), labels.to(device)
#             outputs = model(images)
#             loss = criterion(outputs, labels)

#             total_loss += loss.item() * images.size(0)
#             pred = outputs.argmax(dim=1)
#             correct += (pred == labels).sum().item()
#             total += labels.size(0)

#             preds.extend(pred.cpu().numpy())
#             targets.extend(labels.cpu().numpy())

#     avg_loss = total_loss / total
#     avg_acc = correct / total
#     macro_f1 = f1_score(targets, preds, average='macro')

#     return avg_loss, avg_acc, macro_f1


# # 🟢 Start Training
# train(model, train_loader, val_loader, num_epochs=20)

#코랩 런타임 중단 방지 자바스크립트드
# var startClickConnect = function startClickConnect(){
#     var clickConnect = function clickConnect(){
#         console.log("Connnect Clicked - Start");
#         document.querySelector("#top-toolbar > colab-connect-button").shadowRoot.querySelector("#connect").click();
#         console.log("Connnect Clicked - End");
#     };

#     var intervalId = setInterval(clickConnect, 60000);

#     var stopClickConnectHandler = function stopClickConnect() {
#         console.log("Connnect Clicked Stopped - Start");
#         clearInterval(intervalId);
#         console.log("Connnect Clicked Stopped - End");
#     };

#     return stopClickConnectHandler;
# };

# var stopClickConnect = startClickConnect();