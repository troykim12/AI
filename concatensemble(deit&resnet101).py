# -*- coding: utf-8 -*-
"""concatensemble(deit&resnet101).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfOTeeU-_gc6DbvmI_KMp8_KzL3jIiFb
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from transformers import DeiTModel, DeiTConfig, DeiTFeatureExtractor
from torch.utils.data import DataLoader
from sklearn.metrics import f1_score
from tqdm import tqdm

!pip install Lion

!pip install lion_pytorch

from google.colab import drive
drive.mount('/content/drive')
#êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ë§ˆìš´íŠ¸í•´ì„œ í–ˆìœ¼ë‹ˆê¹Œ ë¡œì»¬ì—ì„œëŠ” í•„ìš”ì—†ì„ê±°ì„

!unzip -q "/content/drive/MyDrive/ê±´ì„¤ìš© ìê°ˆ ë¶„ë¥˜ DACON/train.zip" -d "/content/gravel_data"

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torch.utils.data import DataLoader, Subset

# 1ï¸âƒ£ Transform ì •ì˜
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])

# 2ï¸âƒ£ ImageFolderë¡œ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
dataset = ImageFolder(root="/content/gravel_data/train", transform=transform)

# 3ï¸âƒ£ í´ë˜ìŠ¤ ì´ë¦„ ë° ê°œìˆ˜ ìë™ ì¶”ì¶œ
class_names = dataset.classes
num_classes = len(class_names)
print(f"ğŸ“¦ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
print(f"ğŸ§· í´ë˜ìŠ¤ ëª©ë¡: {class_names}")

# 4ï¸âƒ£ ë¼ë²¨ ëª©ë¡ ì¶”ì¶œ (stratifyìš©)
labels = np.array(dataset.targets)

# 6ï¸âƒ£ Stratified train/val split
train_idx, val_idx = train_test_split(
    np.arange(len(labels)),
    test_size=0.1,
    stratify=labels,
    random_state=42
)

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)

# 7ï¸âƒ£ DataLoader ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=70, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=70, shuffle=False, num_workers=4, pin_memory=True)

# 8ï¸âƒ£ ë¶„í•  ê²°ê³¼ ì¶œë ¥
print(f"âœ… Train: {len(train_dataset)}ê°œ / Val: {len(val_dataset)}ê°œ")

from collections import Counter

train_labels = [labels[i] for i in train_idx]
val_labels = [labels[i] for i in val_idx]

print("Train í´ë˜ìŠ¤ ë¶„í¬:", Counter(train_labels))
print("Val í´ë˜ìŠ¤ ë¶„í¬:", Counter(val_labels))

# âœ… ì´ì–´ì„œ í•™ìŠµì„ ìœ„í•œ ì „ì²´ ì½”ë“œ í…œí”Œë¦¿ (DeiT + ResNet101 ConcatEnsemble)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR
from sklearn.metrics import f1_score
from tqdm import tqdm
import timm

# âš™ï¸ ê¸°ë³¸ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = 7

# ğŸ“ ë°ì´í„° ë¡œë”ëŠ” ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨
# ì˜ˆì‹œ: train_loader, val_loader = get_dataloaders(...)

# ğŸ”¹ ResNet101 Feature Extractor
resnet_base = models.resnet101(pretrained=False)
resnet_ckpt = torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_model_resnet101.pth")
resnet_ckpt = {k: v for k, v in resnet_ckpt.items() if not k.startswith("fc.")}
resnet_base.load_state_dict(resnet_ckpt, strict=False)

class ResNetFeatureExtractor(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.backbone = nn.Sequential(*list(base_model.children())[:-1])
        self.out_dim = base_model.fc.in_features

    def forward(self, x):
        x = self.backbone(x)
        return torch.flatten(x, 1)

resnet = ResNetFeatureExtractor(resnet_base).to(device)

# ğŸ”¹ DeiT Feature Extractor
state_dict = torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_deit_base.pth")
state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}

deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0)
deit.load_state_dict(state_dict)
deit.out_dim = deit.num_features

class DeiTFeatureExtractor(nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.out_dim = backbone.num_features

    def forward(self, x):
        return self.backbone(x)

deit = DeiTFeatureExtractor(deit).to(device)

# ğŸ”¹ Concat Ensemble
class ConcatEnsemble(nn.Module):
    def __init__(self, resnet, deit, num_classes):
        super().__init__()
        self.resnet = resnet
        self.deit = deit
        self.classifier = nn.Sequential(
            nn.Linear(resnet.out_dim + deit.out_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        res_feat = self.resnet(x)
        deit_feat = self.deit(x)
        return self.classifier(torch.cat((res_feat, deit_feat), dim=1))
state_dict = torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_deit_base.pth")

# 2. classifier ë¶€ë¶„ í‚¤ ì œê±° â†’ feature extractor ìš©ìœ¼ë¡œ ì‚¬ìš©
filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}
# timm ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (backbone wrapper ì—†ìŒ)
deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0)
deit.load_state_dict(filtered_state_dict)
deit.out_dim = deit.num_features
deit = deit.to(device)

# ê·¸ëŒ€ë¡œ ì•™ìƒë¸”
model = ConcatEnsemble(resnet, deit, num_classes).to(device)

# ì´ì–´ í•™ìŠµ
model.load_state_dict(torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_concatEnsemble_model.pth"))

start_epoch = 1  # ì—í¬í¬ 1ê¹Œì§€ í•™ìŠµë˜ì—ˆìœ¼ë©´ 2ë¶€í„° ì‹œì‘

# ğŸ”§ Optimizer & Scheduler
optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
warmup_scheduler = LinearLR(optimizer, start_factor=1e-2, total_iters=2 * len(train_loader))

criterion = nn.CrossEntropyLoss()

# ğŸ” Training Loop (ì´ì–´í•˜ê¸° í¬í•¨)
def train(model, train_loader, val_loader, num_epochs, start_epoch=0):
    best_f1 = 0.0
    step = 0
    save_path = "/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_concatEnsemble_model.pth"

    for epoch in range(start_epoch, num_epochs):
        model.train()
        total_loss, correct, total = 0.0, 0, 0

        for images, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}/{num_epochs}]"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            if epoch < 2:
                warmup_scheduler.step()
            else:
                scheduler.step(epoch + step / len(train_loader))
            step += 1

            total_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_loss = total_loss / total
        train_acc = correct / total
        val_loss, val_acc, val_f1 = evaluate(model, val_loader)

        if val_f1 > best_f1:
            best_f1 = val_f1
            torch.save(model.state_dict(), save_path)
            print(f"\nğŸ“¦ Saved new best model at Epoch {epoch+1} with F1: {val_f1:.4f}")

        print(f"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

# ğŸ” Evaluation í•¨ìˆ˜
def evaluate(model, val_loader):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    preds, targets = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            pred = outputs.argmax(dim=1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
            preds.extend(pred.cpu().numpy())
            targets.extend(labels.cpu().numpy())

    avg_loss = total_loss / total
    avg_acc = correct / total
    macro_f1 = f1_score(targets, preds, average='macro')
    return avg_loss, avg_acc, macro_f1

# ğŸŸ¢ ì´ì–´ì„œ í•™ìŠµ ì‹¤í–‰
train(model, train_loader, val_loader, num_epochs=20, start_epoch=start_epoch)

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torchvision import models
# from transformers import DeiTModel, DeiTConfig
# from torch.utils.data import DataLoader
# from torch.optim import AdamW
# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
# from sklearn.metrics import f1_score
# from tqdm import tqdm
# import timm

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# num_classes = 7

# resnet_base = models.resnet101(pretrained=False)
# state_dict = torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_model_resnet101.pth")

# # fc ë ˆì´ì–´ í‚¤ ì œê±°
# state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}

# # ìˆ˜ì •ëœ state_dict ë¡œë“œ
# resnet_base.load_state_dict(state_dict, strict=False)


# # 2ï¸âƒ£ FeatureExtractor í´ë˜ìŠ¤ ì •ì˜
# class ResNetFeatureExtractor(nn.Module):
#     def __init__(self, base_model):
#         super().__init__()
#         self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # avgpool ì „ê¹Œì§€
#         self.out_dim = base_model.fc.in_features

#     def forward(self, x):
#         x = self.backbone(x)
#         x = torch.flatten(x, 1)
#         return x

# # 3ï¸âƒ£ feature extractorì— base ëª¨ë¸ ì£¼ì…
# resnet = ResNetFeatureExtractor(resnet_base).to(device)



# class DeiTFeatureExtractor(nn.Module):
#     def __init__(self, backbone):
#         super().__init__()
#         self.backbone = backbone
#         self.out_dim = backbone.num_features

#     def forward(self, x):
#         return self.backbone(x)


# # ğŸ”¹ Concat Ensemble Model
# class ConcatEnsemble(nn.Module):
#     def __init__(self, resnet, deit, num_classes):
#         super().__init__()
#         self.resnet = resnet
#         self.deit = deit
#         self.classifier = nn.Sequential(
#             nn.Linear(resnet.out_dim + deit.out_dim, 512),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(512, num_classes)
#         )

#     def forward(self, x):
#         res_feat = self.resnet(x)
#         deit_feat = self.deit(x)
#         combined = torch.cat((res_feat, deit_feat), dim=1)
#         return self.classifier(combined)

# # ğŸ”¹ Instantiate Model,ğŸ”¹ Load pretrained weights (.pth)
# state_dict = torch.load("/content/drive/MyDrive/ìê°ˆ train ë° test ì‚¬ì§„/best_deit_base.pth")
# # âŒ head.weight, head.bias í‚¤ ì œê±°
# filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}

# deit = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=0).to(device)
# deit.load_state_dict(filtered_state_dict)

# # âœ… ì´ í•œ ì¤„ë¡œ í•´ê²°ë¨!
# deit.out_dim = deit.num_features
# deit = deit.to(device)

# model = ConcatEnsemble(resnet, deit, num_classes).to(device)



# # ğŸ”¹ Enable fine-tuning
# for p in model.parameters():
#     p.requires_grad = True

# # ğŸ”¹ Optimizer: Lion
# optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)

# # ğŸ”¹ LR Scheduler: CosineAnnealingWarmRestarts
# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)

# # ğŸ”¹ Warmup Scheduler (manual): optional
# warmup_epochs = 2
# warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
#     optimizer, start_factor=1e-2, total_iters=warmup_epochs * len(train_loader)
# )

# # ğŸ”¹ Loss Function
# criterion = nn.CrossEntropyLoss()

# # ğŸ” Training Loop
# def train(model, train_loader, val_loader, num_epochs):
#     step = 0
#     best_f1 = 0.0
#     save_path = "/content/drive/MyDrive/á„Œá…¡á„€á…¡á†¯ train á„†á…µá†¾ test á„‰á…¡á„Œá…µá†«/best_concatEnsemble_model.pth"

#     for epoch in range(num_epochs):
#         model.train()
#         train_loss = 0.0
#         correct = 0
#         total = 0

#         for images, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}/{num_epochs}]"):
#             images, labels = images.to(device), labels.to(device)

#             optimizer.zero_grad()
#             outputs = model(images)
#             loss = criterion(outputs, labels)
#             loss.backward()
#             optimizer.step()

#             # Scheduler & Warmup
#             if epoch < warmup_epochs:
#                 warmup_scheduler.step()
#             else:
#                 scheduler.step(epoch + step / len(train_loader))
#             step += 1

#             train_loss += loss.item() * images.size(0)
#             preds = outputs.argmax(dim=1)
#             correct += (preds == labels).sum().item()
#             total += labels.size(0)

#         epoch_train_loss = train_loss / total
#         epoch_train_acc = correct / total

#         # ğŸ” Validation
#         val_loss, val_acc, val_f1 = evaluate(model, val_loader)

#         # âœ… ëª¨ë¸ ì €ì¥ ì¡°ê±´: F1 ê¸°ì¤€ ìµœê³ 
#         if val_f1 > best_f1:
#             best_f1 = val_f1
#             torch.save(model.state_dict(), save_path)
#             print(f"ğŸ“¦ Saved new best model at Epoch {epoch+1} with F1: {val_f1:.4f}")

#         # ğŸ”¢ ì¶œë ¥
#         print(f"Epoch {epoch+1:02d} | "
#               f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | "
#               f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")



# def evaluate(model, val_loader):
#     model.eval()
#     total_loss = 0.0
#     correct = 0
#     total = 0
#     preds = []
#     targets = []

#     with torch.no_grad():
#         for images, labels in val_loader:
#             images, labels = images.to(device), labels.to(device)
#             outputs = model(images)
#             loss = criterion(outputs, labels)

#             total_loss += loss.item() * images.size(0)
#             pred = outputs.argmax(dim=1)
#             correct += (pred == labels).sum().item()
#             total += labels.size(0)

#             preds.extend(pred.cpu().numpy())
#             targets.extend(labels.cpu().numpy())

#     avg_loss = total_loss / total
#     avg_acc = correct / total
#     macro_f1 = f1_score(targets, preds, average='macro')

#     return avg_loss, avg_acc, macro_f1


# # ğŸŸ¢ Start Training
# train(model, train_loader, val_loader, num_epochs=20)

#ì½”ë© ëŸ°íƒ€ì„ ì¤‘ë‹¨ ë°©ì§€ ìë°”ìŠ¤í¬ë¦½íŠ¸ë“œ
# var startClickConnect = function startClickConnect(){
#     var clickConnect = function clickConnect(){
#         console.log("Connnect Clicked - Start");
#         document.querySelector("#top-toolbar > colab-connect-button").shadowRoot.querySelector("#connect").click();
#         console.log("Connnect Clicked - End");
#     };

#     var intervalId = setInterval(clickConnect, 60000);

#     var stopClickConnectHandler = function stopClickConnect() {
#         console.log("Connnect Clicked Stopped - Start");
#         clearInterval(intervalId);
#         console.log("Connnect Clicked Stopped - End");
#     };

#     return stopClickConnectHandler;
# };

# var stopClickConnect = startClickConnect();